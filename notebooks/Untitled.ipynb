{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875be86d-51e5-4d08-93e7-3d958cd52f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Working dir: /home/sagemaker-user/SageNotebook\n",
      "✅ Using SageMaker Studio execution role\n",
      "Region: us-east-1\n",
      "✅ SageMaker runtime + client ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Step 1: Normalize working dir (repo root)\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "print(\"Working dir:\", pathlib.Path().resolve())\n",
    "\n",
    "# Step 2: Load .env if available (local dev), else fall back to Studio role\n",
    "creds = {}\n",
    "region = None\n",
    "\n",
    "if os.path.exists(\".env\"):\n",
    "    load_dotenv(dotenv_path=\".env\", override=True)\n",
    "    region = os.getenv(\"AWS_REGION\")\n",
    "    creds = {\n",
    "        \"aws_access_key_id\": os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        \"aws_secret_access_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    }\n",
    "    print(\"✅ Loaded AWS creds from .env\")\n",
    "else:\n",
    "    region = sagemaker.Session().boto_region_name\n",
    "    print(\"✅ Using SageMaker Studio execution role\")\n",
    "\n",
    "print(\"Region:\", region)\n",
    "\n",
    "# Step 3: Create SageMaker runtime + client\n",
    "runtime = boto3.client(\"sagemaker-runtime\", region_name=region, **creds)\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region, **creds)\n",
    "\n",
    "print(\"✅ SageMaker runtime + client ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aba9ddf-1cbc-42ff-afe1-43b48637ed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hugging Face config ready\n"
     ]
    }
   ],
   "source": [
    "hub = {\n",
    "    \"HF_MODEL_ID\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"HF_TASK\": \"text-classification\"\n",
    "}\n",
    "print(\"✅ Hugging Face config ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc81206f-f1be-4da8-91eb-3ec62b4fe78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HuggingFace model object created\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version=\"4.26\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    py_version=\"py39\",\n",
    "    env=hub,\n",
    "    role=sagemaker.get_execution_role()\n",
    ")\n",
    "print(\"✅ HuggingFace model object created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b71d46-9afd-44a1-b355-5fa6b6f66d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deleted endpoint: huggingface-sentiment\n",
      "✅ Deleted endpoint config: huggingface-sentiment\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "endpoint_name = \"huggingface-sentiment\"\n",
    "\n",
    "try:\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"✅ Deleted endpoint: {endpoint_name}\")\n",
    "except sm_client.exceptions.ClientError:\n",
    "    print(\"⚠️ Endpoint not found, skipping\")\n",
    "\n",
    "try:\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    print(f\"✅ Deleted endpoint config: {endpoint_name}\")\n",
    "except sm_client.exceptions.ClientError:\n",
    "    print(\"⚠️ Endpoint config not found, skipping\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36d5ab3f-4cbd-44c7-964a-fb1e2e68c647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!✅ Endpoint deployed: huggingface-sentiment\n"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",   \n",
    "    endpoint_name=\"huggingface-sentiment\"\n",
    ")\n",
    "print(\"✅ Endpoint deployed: huggingface-sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "084faa3f-0405-4ba0-9c2c-2986e45aad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [{'label': 'POSITIVE', 'score': 0.9998700618743896}]\n"
     ]
    }
   ],
   "source": [
    "result = predictor.predict({\"inputs\": \"This SageMaker setup is fantastic!\"})\n",
    "print(\"Prediction:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec32633-c8e9-4a07-8fb8-9c0df3752c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Endpoint deleted to save costs\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()\n",
    "print(\"✅ Endpoint deleted to save costs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccefc15e-a2c6-497c-b5d8-8f1866f86151",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "       import os\n",
    "       import pandas as pd\n",
    "       from sklearn.model_selection import train_test_split\n",
    "       from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "       import torch\n",
    "\n",
    "       def load_data(data_dir):\n",
    "           df = pd.read_csv(os.path.join(data_dir, 'banking_complaints.csv'))\n",
    "           df = df.merge(pd.read_csv(os.path.join(data_dir, 'dept_product.csv')).rename(columns={'Product': 'Banking Product', 'Department': 'department'}), on='Banking Product', how='left')\n",
    "           syn_map = {'Credit reporting': 'Credit Reports', 'Credit card': 'Credit Cards', 'Prepaid card': 'Credit Cards', 'Consumer Loan': 'Loans', 'Payday loan': 'Loans', 'Bank account or service': 'CASA', 'Money transfers': 'Remittance'}\n",
    "           df['department'] = df['department'].fillna(df['Banking Product'].map(syn_map))\n",
    "           df = df.dropna(subset=['department']).reset_index(drop=True)\n",
    "           return df\n",
    "\n",
    "       if __name__ == '__main__':\n",
    "           data_dir = '/opt/ml/input/data/training'\n",
    "           df = load_data(data_dir)\n",
    "           tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "           labels = {dept: i for i, dept in enumerate(df['department'].unique())}\n",
    "           df['label'] = df['department'].map(labels)\n",
    "\n",
    "           train_texts, val_texts, train_labels, val_labels = train_test_split(df['Complaint Description'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "           train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
    "           val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "           class ComplaintDataset(torch.utils.data.Dataset):\n",
    "               def __init__(self, encodings, labels):\n",
    "                   self.encodings = encodings\n",
    "                   self.labels = labels\n",
    "               def __getitem__(self, idx):\n",
    "                   item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "                   item['labels'] = torch.tensor(self.labels[idx])\n",
    "                   return item\n",
    "               def __len__(self):\n",
    "                   return len(self.labels)\n",
    "\n",
    "           train_dataset = ComplaintDataset(train_encodings, train_labels.tolist())\n",
    "           val_dataset = ComplaintDataset(val_encodings, val_labels.tolist())\n",
    "\n",
    "           model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(labels))\n",
    "           training_args = TrainingArguments(\n",
    "               output_dir='/opt/ml/model',\n",
    "               num_train_epochs=3,\n",
    "               per_device_train_batch_size=16,\n",
    "               per_device_eval_batch_size=16,\n",
    "               warmup_steps=500,\n",
    "               weight_decay=0.01,\n",
    "               logging_dir='/opt/ml/logs',\n",
    "               logging_steps=10,\n",
    "               evaluation_strategy='epoch',\n",
    "               save_strategy='epoch',\n",
    "               load_best_model_at_end=True\n",
    "           )\n",
    "\n",
    "           trainer = Trainer(\n",
    "               model=model,\n",
    "               args=training_args,\n",
    "               train_dataset=train_dataset,\n",
    "               eval_dataset=val_dataset\n",
    "           )\n",
    "\n",
    "           trainer.train()\n",
    "           trainer.save_model('/opt/ml/model')\n",
    "       ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
